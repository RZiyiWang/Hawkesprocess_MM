{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tf_agents.environments import tf_py_environment\n",
    "import copy\n",
    "import numpy as np\n",
    "from tf_agents.specs import BoundedArraySpec, BoundedTensorSpec\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "from tf_agents.trajectories.time_step import TimeStep\n",
    "\n",
    "from adversaryEnv import *\n",
    "from continualAgent import *\n",
    "from allowNotQuoteEnv import *\n",
    "from constants import *\n",
    "from dynamics import *\n",
    "from marketMakerEnv import *\n",
    "from QuoteAgent import *\n",
    "from strategies import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names(eta, zeta,experiment_number=\"Hawkes1\",adversary=\"All\"):\n",
    "    eta_str = str(eta)\n",
    "    zeta_str = str(zeta)\n",
    "\n",
    "    adversary_name = \"adversary_\" + experiment_number + \"_eta=\" + eta_str + \"_zeta=\" + zeta_str + \"_\" + adversary\n",
    "    adversary_policyname = adversary_name+'_saved_policy'\n",
    "\n",
    "    MM_name = \"MM_\" + experiment_number + \"_eta=\" + eta_str + \"_zeta=\" + zeta_str + \"_\" + adversary\n",
    "    MM_policyname = MM_name+'_saved_policy'\n",
    "\n",
    "    MM_2actions_name = \"2actionMM_\" + experiment_number + \"_eta=\" + eta_str + \"_zeta=\" + zeta_str + \"_\" + adversary\n",
    "    MM_2actions_policyname = MM_2actions_name+'_saved_policy'\n",
    "\n",
    "    MM_4actions_name = \"4actionMM_\" + experiment_number + \"_eta=\" + eta_str + \"_zeta=\" + zeta_str + \"_\" + adversary\n",
    "    MM_4actions_policyname = MM_4actions_name+'_saved_policy'\n",
    "\n",
    "    return (adversary_name, adversary_policyname,\n",
    "            MM_name, MM_policyname,\n",
    "            MM_2actions_name, MM_2actions_policyname,\n",
    "            MM_4actions_name, MM_4actions_policyname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "logging.set_verbosity(logging.INFO)\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversary_agent(env_class, eta, zeta, num_iterations,experiment_number=\"Hawkes1\",adversary=\"All\",continue_training=False, continue_policy=None):\n",
    "\n",
    "    experiment_names = names(experiment_number=experiment_number, eta=eta, zeta=zeta, adversary=adversary)\n",
    "\n",
    "    adversary_name = experiment_names[0]\n",
    "    adversary_policyname = experiment_names[1]\n",
    "    MM_name = experiment_names[2]\n",
    "    MM_policyname = experiment_names[3]\n",
    "    MM_2actions_name = experiment_names[4]\n",
    "    MM_2actions_policyname = experiment_names[5]\n",
    "    MM_4actions_name = experiment_names[6]\n",
    "    MM_4actions_policyname = experiment_names[7]\n",
    "\n",
    "    adversary_env = env_class(eta=eta, zeta=zeta)\n",
    "    adversary_tf_env = tf_py_environment.TFPyEnvironment(adversary_env)\n",
    "    adversary_eval_env = tf_py_environment.TFPyEnvironment(adversary_env)\n",
    "\n",
    "    if continue_training and continue_policy is not None:\n",
    "        adversary_agent = Agent(adversary_tf_env, adversary_eval_env, name=adversary_name,continue_saved_policy=continue_policy, initialize_replay_buffer=False)\n",
    "    else:\n",
    "        adversary_agent = Agent(adversary_tf_env, adversary_eval_env, name=adversary_name)\n",
    "    \n",
    "    adversary_agent.train(num_iterations)\n",
    "\n",
    "    adversary_saved_policy = tf.saved_model.load(adversary_policyname)\n",
    "\n",
    "    adversary_results = evaluation(policy=adversary_saved_policy, name=adversary_name, env=adversary_env, calculate_ratio=False, num_episodes=1000,num_times=100)\n",
    "\n",
    "    adversary_validate_results = validate_with_random_policy(name=adversary_name, env=adversary_env,num_episodes=1000,num_times=100)\n",
    "\n",
    "    return adversary_results, adversary_validate_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta=0.0\n",
    "zeta=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary_resultsALL, adversary_validate_resultsALL = train_adversary_agent(AdversaryEnvironmentWithControllingAll, eta, zeta,num_iterations=50000,experiment_number=\"Hawkes2\",adversary=\"All\",continue_training=True, continue_policy='adversary_Hawkes1_eta=0.0_zeta=0.0_All_saved_policy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
